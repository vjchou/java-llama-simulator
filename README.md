# Java Llama Simulator

åŸºæ–¼ llama.cpp çš„ Java èªè¨€æ¨¡å‹æ¨¡æ“¬å™¨ï¼Œé€é JNI ç¶å®šå¯¦ç¾ Java èˆ‡åŸç”Ÿ C++ çš„é«˜æ•ˆæ•´åˆã€‚

## ç›®éŒ„

- [å°ˆæ¡ˆæ¦‚è¿°](#å°ˆæ¡ˆæ¦‚è¿°)
- [ç³»çµ±æ¶æ§‹](#ç³»çµ±æ¶æ§‹)
- [ç’°å¢ƒéœ€æ±‚](#ç’°å¢ƒéœ€æ±‚)
- [ä¾è³´æº–å‚™](#ä¾è³´æº–å‚™)
- [æ ¸å¿ƒçµ„ä»¶](#æ ¸å¿ƒçµ„ä»¶)
- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
- [åƒè€ƒè³‡æº](#åƒè€ƒè³‡æº)

## å°ˆæ¡ˆæ¦‚è¿°

æœ¬å°ˆæ¡ˆæ•´åˆ llama.cpp èˆ‡ Javaï¼Œé€é java-llama.cpp çš„ JNI ç¶å®šï¼Œè®“ Java æ‡‰ç”¨ç¨‹å¼èƒ½å¤ ç›´æ¥ä½¿ç”¨ GGUF æ ¼å¼çš„èªè¨€æ¨¡å‹é€²è¡Œæ¨ç†ã€‚
é©ç”¨æ–¼éœ€è¦åœ¨ Java ç”Ÿæ…‹ç³»çµ±ä¸­éƒ¨ç½²æœ¬åœ° LLM çš„å ´æ™¯ã€‚

## ç³»çµ±æ¶æ§‹

```bash
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Java Main                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ModelParams â”‚â”€â”€â”€â–¶â”‚ LlamaModel  â”‚â—€â”€â”€â”€â”‚InferParams  â”‚  â”‚
â”‚  â”‚ (æ¨¡å‹é…ç½®)   â”‚    â”‚ (JNI ç¶å®š)   â”‚    â”‚ (æ¨ç†é…ç½®)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚         â–¼                  â–¼                  â–¼         â”‚
â”‚    generate()         complete()          embed()       â”‚
â”‚    (ä¸²æµç”Ÿæˆ)          (å®Œæ•´ç”Ÿæˆ)          (å‘é‡åµŒå…¥)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼ JNI
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   llama.cpp     â”‚
                    â”‚  (Native C++)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   GGUF æ¨¡å‹æª”    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ¶æ§‹èªªæ˜

- **Java Layer**: æ‡‰ç”¨ç¨‹å¼é‚è¼¯å±¤ï¼Œä½¿ç”¨ java-llama.cpp æä¾›çš„ API
- **JNI Bridge**: é€é JNI é€£æ¥ Java èˆ‡åŸç”Ÿ C++ ç¨‹å¼ç¢¼
- **Native Layer**: llama.cpp åŸç”ŸåŸ·è¡Œå¼•æ“ï¼Œè² è²¬å¯¦éš›çš„æ¨¡å‹æ¨ç†
- **Model Layer**: GGUF æ ¼å¼çš„é‡åŒ–æ¨¡å‹æª”æ¡ˆ

## ç’°å¢ƒéœ€æ±‚

- **Java**: JDK 17 æˆ–ä»¥ä¸Šç‰ˆæœ¬
- **ä½œæ¥­ç³»çµ±**: Windows 11 / Linux / macOS
- **å»ºæ§‹å·¥å…·**: Maven 3.6+
- **è¨˜æ†¶é«”**: å»ºè­° 8GB ä»¥ä¸Šï¼ˆè¦–æ¨¡å‹å¤§å°è€Œå®šï¼‰

## ä¾è³´æº–å‚™

### 1. å–å¾— llama.cpp

å¾å¯¦ä½œè§’åº¦ä¾†çœ‹ï¼Œæˆ‘å€‘éœ€è¦çš„æ˜¯**å¯ä»¥ç›´æ¥åŸ·è¡Œçš„é ç·¨è­¯å·¥å…·**ï¼Œè€Œéå¾åŸå§‹ç¢¼ç·¨è­¯ã€‚

#### ä¸‹è¼‰æ­¥é©Ÿ

1. **å‰å¾€å°ˆæ¡ˆé é¢**  
   [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)

2. **é€²å…¥ Releases é ç±¤**  
   é»é¸å³å´çš„ "Releases" æŸ¥çœ‹æ‰€æœ‰ç™¼å¸ƒç‰ˆæœ¬

3. **é¸æ“‡å°æ‡‰å¹³å°çš„ç‰ˆæœ¬**
   
   | å¹³å° | æª”æ¡ˆç¯„ä¾‹ |
   |------|---------|
   | Windows (CPU) | `llama-b7751-bin-win-cpu-x64.zip` |
   | Windows (CUDA) | `llama-b7751-bin-win-cuda-cu12.2.0-x64.zip` |
   | Linux (CPU) | `llama-b7751-bin-linux-x64.zip` |
   | macOS (ARM) | `llama-b7751-bin-macos-arm64.zip` |

4. **è§£å£“ç¸®æª”æ¡ˆ**  
   è§£å£“åˆ°å°ˆæ¡ˆç›®éŒ„æˆ–ç³»çµ±è·¯å¾‘

> âš ï¸ **ç‰ˆæœ¬æ³¨æ„äº‹é …**  
> llama.cpp æ›´æ–°é€Ÿåº¦å¾ˆå¿«ï¼Œç‰ˆæœ¬è™Ÿï¼ˆå¦‚ `b7751`ï¼‰æœƒæŒçºŒè®Šå‹•ï¼Œè«‹ä»¥å¯¦éš›å·²é‡‹å‡ºçš„æœ€æ–°ç‰ˆæœ¬ç‚ºä¸»ã€‚

> ğŸ’¡ **GGUF æ ¼å¼è½‰æ›**  
> å¦‚éœ€è‡ªè¡Œè½‰æ› GGUF æ ¼å¼ï¼Œéœ€å…‹éš† llama.cpp å€‰åº«ä¸¦ä½¿ç”¨ Python è…³æœ¬è™•ç†ï¼Œæ­¤éƒ¨åˆ†ä¸åœ¨æœ¬å°ˆæ¡ˆç¯„åœå…§ã€‚

### 2. æ•´åˆ java-llama.cpp

**å°ˆæ¡ˆä½ç½®**: [https://github.com/kherud/java-llama.cpp](https://github.com/kherud/java-llama.cpp)

java-llama.cpp å°è£äº† llama.cpp çš„ JNI ç¶å®šï¼Œè®“ Java æ‡‰ç”¨ç¨‹å¼èƒ½å¤ ç›´æ¥èª¿ç”¨åŸç”Ÿæ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚

#### Maven ä¾è³´é…ç½®

åœ¨ `pom.xml` ä¸­åŠ å…¥ä»¥ä¸‹ä¾è³´ï¼š

```xml
<!-- Source: https://mvnrepository.com/artifact/de.kherud/llama -->
<dependency>
    <groupId>de.kherud</groupId>
    <artifactId>llama</artifactId>
    <version>4.2.0</version>
</dependency>
```

> ğŸ“Œ è«‹æŸ¥çœ‹ [java-llama.cpp releases](https://github.com/kherud/java-llama.cpp/releases) ä»¥ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ã€‚

## æ ¸å¿ƒçµ„ä»¶

### ModelParamsï¼ˆæ¨¡å‹é…ç½®ï¼‰

è² è²¬è¨­å®šæ¨¡å‹è¼‰å…¥æ™‚çš„åƒæ•¸ï¼š

```java
ModelParameters params = new ModelParameters()
    .setNGpuLayers(35)        // GPU åŠ é€Ÿå±¤æ•¸
    .setContextSize(2048)     // ä¸Šä¸‹æ–‡å¤§å°
    .setSeed(42);             // éš¨æ©Ÿç¨®å­
```

**ä¸»è¦é…ç½®é …**ï¼š
- `nGpuLayers`: GPU è™•ç†çš„å±¤æ•¸ï¼ˆ0 = ç´” CPUï¼‰
- `contextSize`: ä¸Šä¸‹æ–‡çª—å£å¤§å°
- `seed`: éš¨æ©Ÿç¨®å­ï¼Œå›ºå®šè¼¸å‡ºçµæœ
- `threads`: CPU åŸ·è¡Œç·’æ•¸

### LlamaModelï¼ˆJNI ç¶å®šï¼‰

æ ¸å¿ƒæ¨¡å‹é¡åˆ¥ï¼Œé€é JNI èˆ‡åŸç”Ÿ llama.cpp äº¤äº’ï¼š

```java
LlamaModel model = new LlamaModel("path/to/model.gguf", params);
```

**æä¾›ä¸‰ç¨®æ¨ç†æ–¹æ³•**ï¼š
- `generate()`: ä¸²æµç”Ÿæˆ
- `complete()`: å®Œæ•´ç”Ÿæˆ
- `embed()`: å‘é‡åµŒå…¥

### InferParamsï¼ˆæ¨ç†é…ç½®ï¼‰

è¨­å®šæ¨ç†æ™‚çš„åƒæ•¸ï¼Œæ§åˆ¶ç”Ÿæˆå“è³ªèˆ‡å¤šæ¨£æ€§ï¼š

```java
InferenceParameters inferParams = new InferenceParameters("")
    .setTemperature(0.7f)     // æº«åº¦ï¼ˆå‰µé€ æ€§ï¼‰
    .setTopP(0.9f)            // æ ¸æ¡æ¨£
    .setTopK(40)              // Top-K æ¡æ¨£
    .setPenalizeNl(false);    // æ›è¡Œæ‡²ç½°
```

**ä¸»è¦åƒæ•¸èªªæ˜**ï¼š
- `temperature`: 0.0-1.0ï¼Œè¶Šé«˜è¶Šæœ‰å‰µé€ æ€§ï¼Œè¶Šä½è¶Šç¢ºå®š
- `topP`: æ ¸æ¡æ¨£é–¾å€¼ï¼Œæ§åˆ¶è©å½™å¤šæ¨£æ€§
- `topK`: é™åˆ¶å€™é¸è©æ•¸é‡
- `penalizeNl`: æ˜¯å¦æ‡²ç½°æ›è¡Œç¬¦è™Ÿ

## åŠŸèƒ½ç‰¹æ€§

### 1. generate() - ä¸²æµç”Ÿæˆ

**é©ç”¨å ´æ™¯**ï¼šèŠå¤©æ©Ÿå™¨äººã€å³æ™‚å°è©±ç³»çµ±

**ç‰¹é»**ï¼š
- é€å­—è¼¸å‡ºï¼Œå³æ™‚åé¥‹
- é©åˆéœ€è¦å±•ç¤ºç”Ÿæˆéç¨‹çš„æ‡‰ç”¨
- å¯ä¸­æ–·ç”Ÿæˆ

### 2. complete() - å®Œæ•´ç”Ÿæˆ

**é©ç”¨å ´æ™¯**ï¼šæ‰¹æ¬¡è™•ç†ã€æ–‡ç« ç”Ÿæˆã€ç¨‹å¼ç¢¼è£œå…¨

**ç‰¹é»**ï¼š
- ä¸€æ¬¡æ€§è¿”å›å®Œæ•´çµæœ
- é©åˆæ‰¹æ¬¡ä»»å‹™
- æ•ˆèƒ½è¼ƒå„ª

### 3. embed() - å‘é‡åµŒå…¥

**é©ç”¨å ´æ™¯**ï¼šèªç¾©æœç´¢ã€RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰ã€æ–‡æœ¬ç›¸ä¼¼åº¦è¨ˆç®—

**ç‰¹é»**ï¼š
- å°‡æ–‡æœ¬è½‰æ›ç‚ºé«˜ç¶­å‘é‡
- æ”¯æ´èªç¾©ç´šåˆ¥çš„æ–‡æœ¬æ¯”è¼ƒ
- RAG ç³»çµ±çš„åŸºç¤çµ„ä»¶

## å¿«é€Ÿé–‹å§‹

### 1. å…‹éš†å°ˆæ¡ˆ

```bash
git clone <repository-url>
cd java-llama-simulator
```

### 2. ç·¨è­¯å°ˆæ¡ˆ

```bash
mvn clean compile
```

### 3. åŸ·è¡Œæ¸¬è©¦

```bash
mvn test
```

### 4. æ‰“åŒ…å°ˆæ¡ˆ

```bash
mvn package
```

### 5. åŸ·è¡Œæ‡‰ç”¨

```bash
java -jar target/java-llama-simulator.jar
```

## ä½¿ç”¨ç¯„ä¾‹

### åŸºæœ¬å°è©±

```java
public class SimpleChat {
    public static void main(String[] args) {
        ModelParameters modelParams = new ModelParameters()
            .setContextSize(2048);
        
        LlamaModel model = new LlamaModel("models/llama-2-7b.gguf", modelParams);
        
        InferenceParameters inferParams = new InferenceParameters("ä½ å¥½ï¼")
            .setTemperature(0.7f);
        
        for (LlamaOutput output : model.generate(inferParams)) {
            System.out.print(output);
        }
        
        model.close();
    }
}
```

### RAG ç³»çµ±ç¯„ä¾‹

åƒè€ƒ [WeatherRagDemo.java](src/test/java/omni/simulator/llama/WeatherRagDemo.java) äº†è§£å¦‚ä½•å¯¦ä½œæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±ã€‚

## å¸¸è¦‹å•é¡Œ

### Q1: å¦‚ä½•é¸æ“‡åˆé©çš„æ¨¡å‹ï¼Ÿ

- **7B æ¨¡å‹**: é©åˆä¸€èˆ¬å°è©±ã€å•ç­”ï¼ˆéœ€è¦ 8GB+ RAMï¼‰
- **13B æ¨¡å‹**: æ›´å¥½çš„ç†è§£èƒ½åŠ›ï¼ˆéœ€è¦ 16GB+ RAMï¼‰
- **é‡åŒ–ç‰ˆæœ¬**: Q4_K_M æ˜¯å“è³ªèˆ‡æ•ˆèƒ½çš„å¹³è¡¡é»

### Q2: GPU åŠ é€Ÿå¦‚ä½•é…ç½®ï¼Ÿ

è¨­å®š `nGpuLayers` åƒæ•¸ï¼š
- å€¼è¶Šå¤§ï¼Œä½¿ç”¨ GPU è¶Šå¤š
- 0 = ç´” CPU é‹ç®—
- -1 = æ‰€æœ‰å±¤éƒ½ç”¨ GPU

### Q3: è¨˜æ†¶é«”ä¸è¶³æ€éº¼è¾¦ï¼Ÿ

- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ 3Bï¼‰
- é™ä½ `contextSize`
- ä½¿ç”¨æ›´é«˜å£“ç¸®çš„é‡åŒ–ç‰ˆæœ¬ï¼ˆQ2_Kï¼‰

## åƒè€ƒè³‡æº

- [llama.cpp å®˜æ–¹å€‰åº«](https://github.com/ggml-org/llama.cpp)
- [java-llama.cpp å®˜æ–¹å€‰åº«](https://github.com/kherud/java-llama.cpp)
- [GGUF æ ¼å¼è¦ç¯„](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [Hugging Face æ¨¡å‹åº«](https://huggingface.co/models?library=gguf)
